import os
import asyncio
import httpx
import unicodedata
import re
from bs4 import BeautifulSoup, Tag, NavigableString
import pandas as pd
from urllib.parse import urljoin
import json

# Carrega configurações
CONFIG_PATH = os.path.join(os.path.dirname(__file__), '..', 'configuracoes', 'configuração diretórios.json')
with open(CONFIG_PATH, encoding="utf-8") as f:
    config = json.load(f)

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CSV_DIR = os.path.join(BASE_DIR, config["CSV_DIR"])
CSV_PATTERN = config["CSV_PATTERN"]

url_base = "https://rickatacado.com"
headers = {
    "User-Agent": "Mozilla/5.0"
}

def limpar_espacos(txt):
    if txt is None:
        return None
    s = txt.replace("\xa0", " ")
    return re.sub(r"\s+", " ", s).strip()

def normalizar_texto(txt):
    if not txt:
        return ""
    txt = unicodedata.normalize('NFKD', txt)
    txt = "".join([c for c in txt if not unicodedata.combining(c)])
    return txt.lower().strip()

def limpar_nome_marca(nome):
    # Remove prefixos como "Tênis Atacado", "Tenis Atacado", etc, e deixa maiúsculo
    nome = re.sub(r"^(t[eê]nis\s+atacado\s*)", "", nome, flags=re.IGNORECASE)
    return nome.strip().upper()

def padronizar_modelo(modelo):
    return modelo.strip().upper() if modelo else ""

def encontrar_titulo_mais_proximo(tag):
    while tag:
        prev = tag.find_previous(class_="wp-block-heading")
        if prev:
            return limpar_espacos(prev.get_text())
        tag = tag.parent
    return None

def encontrar_numeracao_mais_proxima(tag):
    padrao_numeracao = re.compile(
        r"numerac[aã]o(?:es)?[:\s\-–—]*([^\n\r<]+)", re.IGNORECASE
    )
    padrao_dupla = re.compile(r"\b(\d{2}[/\-–—]\d{2})\b")
    padrao_intervalo = re.compile(r"\b(\d{2})\s*[-–—]\s*(\d{2})\b")

    atual = tag
    while atual:
        anterior = atual.find_previous(string=True)
        if anterior:
            texto = normalizar_texto(anterior)
            match = padrao_numeracao.search(texto)
            if match:
                trecho = match.group(1).strip()
                duplas = padrao_dupla.findall(trecho)
                if duplas:
                    if len(duplas) == 1:
                        return duplas[0]
                    else:
                        return f"{duplas[0]} {duplas[-1]}"
                intervalo = padrao_intervalo.search(trecho)
                if intervalo:
                    return f"{intervalo.group(1)}-{intervalo.group(2)}"
                return trecho
            duplas = padrao_dupla.findall(texto)
            if duplas:
                if len(duplas) == 1:
                    return duplas[0]
                else:
                    return f"{duplas[0]} {duplas[-1]}"
            intervalo = padrao_intervalo.search(texto)
            if intervalo:
                return f"{intervalo.group(1)}-{intervalo.group(2)}"
        atual = atual.parent
    return ""

def montar_bloco_texto_entre_titulos(titulo_tag):
    conteudo = []
    for sib in titulo_tag.next_siblings:
        if isinstance(sib, Tag) and "wp-block-heading" in sib.get("class", []):
            break
        conteudo.append(sib)
    textos = []
    for t in conteudo:
        if isinstance(t, NavigableString):
            s = limpar_espacos(str(t))
            if s:
                textos.append(s)
        elif isinstance(t, Tag):
            s = limpar_espacos(t.get_text(" ", strip=True))
            if s:
                textos.append(s)
    return limpar_espacos(" ".join(textos)) or ""

def encontrar_precos_por_modelo(site_content):
    precos = {}
    for heading in site_content.find_all(class_="wp-block-heading"):
        bloco_texto = montar_bloco_texto_entre_titulos(heading)
        preco_atacado = ""
        match_atacado = re.search(r"atacado[^\d]{0,10}(R\$\s*\d{1,3}(?:\.\d{3})*,\s*0{0,1}\d{2})", bloco_texto, re.IGNORECASE)
        if match_atacado:
            preco_atacado = match_atacado.group(1)
        preco_varejo = ""
        match_varejo = re.search(r"varejo[^\d]{0,10}(R\$\s*\d{1,3}(?:\.\d{3})*,\s*0{0,1}\d{2})", bloco_texto, re.IGNORECASE)
        if match_varejo:
            preco_varejo = match_varejo.group(1)
        modelo = limpar_espacos(heading.get_text())
        precos[modelo] = (preco_atacado, preco_varejo)
    return precos

async def fetch(client, url):
    try:
        resp = await client.get(url, timeout=20)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f"Erro ao acessar {url}: {e}")
        return None

async def processar_marca(client, a):
    nome_marca = limpar_nome_marca(a.get_text(strip=True))
    url_marca = a["href"]
    if not url_marca.startswith("http"):
        url_marca = httpx.URL(url_base).join(url_marca)
    html = await fetch(client, url_marca)
    linhas = []
    if html:
        soup = BeautifulSoup(html, "html.parser")
        site_content = soup.find(id="site-content")
        if site_content:
            precos_modelo = encontrar_precos_por_modelo(site_content)
            for row in site_content.find_all("div", class_="tiled-gallery__row columns-2"):
                modelo = encontrar_titulo_mais_proximo(row)
                numeracao = encontrar_numeracao_mais_proxima(row)
                preco_atacado, preco_varejo = precos_modelo.get(modelo, ("", ""))
                if not modelo:
                    continue
                cols = row.find_all("div", class_="tiled-gallery__col")
                link_imagem1 = ""
                link_imagem2 = ""
                if len(cols) > 0:
                    img1 = cols[0].find("img")
                    if img1:
                        srcset1 = img1.get("srcset")
                        if srcset1:
                            link_imagem1 = urljoin(str(url_marca), srcset1.split(",")[-1].split()[0].strip())
                        else:
                            orig1 = img1.get("data-orig-file")
                            if orig1:
                                link_imagem1 = urljoin(str(url_marca), orig1.strip())
                if len(cols) > 1:
                    img2 = cols[1].find("img")
                    if img2:
                        srcset2 = img2.get("srcset")
                        if srcset2:
                            link_imagem2 = urljoin(str(url_marca), srcset2.split(",")[-1].split()[0].strip())
                        else:
                            orig2 = img2.get("data-orig-file")
                            if orig2:
                                link_imagem2 = urljoin(str(url_marca), orig2.strip())
                linhas.append({
                    "marca": nome_marca,
                    "modelo": padronizar_modelo(modelo),
                    "numeracao": numeracao,
                    "preco_atacado": preco_atacado,
                    "preco_varejo": preco_varejo,
                    "link_imagem1": link_imagem1,
                    "link_imagem2": link_imagem2
                })
    print(f"Modelos coletados para {nome_marca}")
    return linhas

async def main():
    import requests
    response = requests.get(url_base, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    menu_marcas = soup.select_one('li#menu-item-14103 ul.sub-menu')

    linhas = []
    if menu_marcas:
        async with httpx.AsyncClient(headers=headers, follow_redirects=True) as client:
            sem = asyncio.Semaphore(8)
            async def sem_task(a):
                async with sem:
                    return await processar_marca(client, a)
            tasks = [sem_task(a) for a in menu_marcas.select("a")]
            resultados = await asyncio.gather(*tasks)
            for modelos in resultados:
                linhas.extend(modelos)
        df = pd.DataFrame(linhas)
        # Garante que marca e modelo estão padronizados (caso algum escape)
        df["marca"] = df["marca"].astype(str).apply(limpar_nome_marca)
        df["modelo"] = df["modelo"].astype(str).apply(padronizar_modelo)
        # Adiciona as colunas min e max
        def extrair_min_max(numeracao):
            if not isinstance(numeracao, str):
                return None, None
            pares = re.findall(r"(\d{2})\s*[/]\s*(\d{2})", numeracao)
            if pares:
                pares_str = [f"{a}/{b}" for a, b in pares]
                return pares_str[0], pares_str[-1]
            m = re.search(r"(\d{2})\s*(?:-|–|ao|até)\s*(\d{2})", numeracao, re.IGNORECASE)
            if m:
                return int(m.group(1)), int(m.group(2))
            return None, None

        df["min"], df["max"] = zip(*df["numeracao"].map(extrair_min_max))
        csv_name = CSV_PATTERN
        csv_path = os.path.join(CSV_DIR, csv_name)
        os.makedirs(CSV_DIR, exist_ok=True)
        if os.path.exists(csv_path):
            df_antigo = pd.read_csv(csv_path)
            cols_base = list(df.columns)
            # Compara só as colunas base
            if set(cols_base).issubset(df_antigo.columns):
                df_antigo_base = df_antigo[cols_base]
            else:
                df_antigo_base = df_antigo
            df_compare = df.sort_values(by=cols_base).reset_index(drop=True)
            df_antigo_compare = df_antigo_base.sort_values(by=cols_base).reset_index(drop=True)
            if df_compare.equals(df_antigo_compare):
                print("Nenhuma alteração detectada. Pipeline não será executado.")
                return
            # Houve alteração: atualiza só as colunas base, preservando as extras
            for col in cols_base:
                df_antigo[col] = df[col]
            df_antigo.to_csv(csv_path, index=False, encoding="utf-8-sig")
            print(f"CSV '{csv_path}' atualizado (mantendo colunas extras)!")
        else:
            # Não existe CSV ainda, salva normalmente
            df.to_csv(csv_path, index=False, encoding="utf-8-sig")
            print(f"CSV '{csv_path}' criado com sucesso!")
    else:
        print("Menu de marcas não encontrado!")

if __name__ == "__main__":
    asyncio.run(main())